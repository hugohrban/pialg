{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zpětnovazební učení"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto cvičení budeme pracovat s Open AI gym, což je open source rozhraní určené pro úkoly zpětnovazebního učení. Jeho hlavní výhodou je, že implementace různých typů algoritmů pro zpětnovazební učení je v něm celkem jednoduchá. Popis základních funkcí Open AI gym se nachází v kódu níž.\n",
    "\n",
    "Dnešní úkol bude naimplementovat agenta, který se učí chovat v nějakém prostředí (konkrétně v MountainCar) pomocí Q-učení.\n",
    "\n",
    "Q-učení je způsob, kdy se agent učí svou strategii, jak se chovat v daném prostředí, pomocí zpětné vazby, kterou od prostředí za své chování dostává. Na rozdíl od hladového agenta, který jen v každém stavu vybírá nový stav na základě akce, co maximalizuje jeho užitek, bere v potaz to, že mezi stavy existují vztahy, které jsou dány Bellmanovými rovnicemi.\n",
    "\n",
    "Nyní se tedy podíváme na příklad autíčka,které se snaží dostat do cíle, ale zatím se pohybuje náhodně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukážeme si, jak si vytvořit jednoduché prostředí *MountainCar*: https://gym.openai.com/envs/MountainCar-v0. \n",
    "\n",
    "Cílem je, aby se autíčko dostalo z údolí až nahoru k vlaječce, ale nemá dost silný motorek, takže se musí nejprve rozhoupat, aby tam vyjelo. V této základní verzi je zde v každém stavu náhodně zvolena akce pro pohyb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(100):\n",
    "   action = env.action_space.sample()\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jen tak pro zajímavost existuje i například prostředí *CartPole*, kde je zase cílem vyvažovat tyčku, aby nespadla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugohrban/Documents/pialg/venv/lib/python3.9/site-packages/gymnasium/envs/registration.py:578: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0', render_mode='human')\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(100):\n",
    "   action = env.action_space.sample()\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opět si vytvoříme MountainCar prostředí. Můžeme si vypsat informace o rozměrech prostoru pozorování a akcí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "observation space low: [-1.2  -0.07]\n",
      "observation space high: [0.6  0.07]\n",
      "action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('observation space low:', env.observation_space.low)\n",
    "print('observation space high:', env.observation_space.high)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Před spuštěním simulace prostředí je potřeba ho resetovat a dostaneme se do prvního pozorování, tedy počátečního stavu s počáteční náhodnou pozicí blízko minima údolí a nulovou rychlostí. Potom je potřeba v prostředí udělat nějakou akci. Třeba opět náhodnou, která se nám koneckonců bude za chvíli hodit pro náhodného agenta. Tu uděláme pomocí metody ```sample()```. Samotný krok, tedy vykonání akce v prostředí a posun do dalšího stavu, uděláme pomocí metody ```step(action)``` a získáme nové pozorování, o kterém si můžeme vypsat další užitečné informace jako odměnu, informaci o konci simulace a další info.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation: (array([-0.4452088,  0.       ], dtype=float32), {})\n",
      "next observation: [-0.44679132 -0.00158252]\n",
      "reward: -1.0\n",
      "terminated: False\n",
      "truncated: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset(seed=42)\n",
    "print('initial observation:', observation)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print('next observation:', observation)\n",
    "print('reward:', reward)\n",
    "print('terminated:', terminated)\n",
    "print('truncated:', truncated)\n",
    "print('info:', info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použijeme nyní kód výše a vytvoříme si do něj třídu pro agenta, který se v prostředí zatím bude chovat náhodně, což nám sice v tuto chvíli nepřinese nic užitečného, ale můžete ho později použít jako základ pro zpětnovazebního agenta.\n",
    "\n",
    "Stav agenta dvojice je pozice a rychlost, akce může být pohyb vlevo, vpravo a nebo se nepohnout. Agent bude mít dvě metody, jednu na to, aby věděl, jak se má chovat a druhou aby se uměl resetovat. Agenta totiž budeme trénovat v několika iteracích. Pro zajímavost si také vypíšeme, jak vypadá prostředí, ve kterém se agent pohybuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obecna trida pro agenta\n",
    "class RandomAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.train = True\n",
    "    \n",
    "    def act(self, observe, reward, done):\n",
    "        return self.actions.sample()\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusíme si napsat trénovací cyklus. Každá iterace for cyklu je jedna hra s novým náhodným začátkem (kolem minima). Ve while cyklu se trénují přechody mezi stavy agenta. Zároveň si pamatujeme celkovou odměnu a číslo kroku (jeden krok je provedení jedné akce), které se nám bude hodit pro logování trénovacích cyklu. K tomu, aby se agent mohl něco učit musí získávat od prostředí nějakou odměnu. V tomto příkladu by měl agent dostávat v každém kroku odměnu -1, když není v cílovém stavu a 0 pokud v něm je. Snižující se suma odměn totiž agenta nutí, aby prohledával prostředí, a tedy vyjel nahoru co nejdříve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(env.action_space)\n",
    "total_rewards = []\n",
    "for i in range(1000):\n",
    "    observation, info = env.reset()\n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    terminated = False\n",
    "    reward = 0\n",
    "    total_reward = 0 # celkova odmena - jen pro logovani\n",
    "    time = 0 # cislo kroku - jen pro logovani\n",
    "    \n",
    "    while not (done or terminated):\n",
    "        action = agent.act(observation, reward, done or terminated)\n",
    "        observation, reward, done, terminated, _ = env.step(action) \n",
    "        total_reward += reward\n",
    "        time += 1\n",
    "            \n",
    "        \n",
    "    total_rewards.append(total_reward)\n",
    "    print(f\"Iteration: {i}, reward: {total_reward}\")\n",
    "agent.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na konec si zobrazíme animaci a graf učení, abychom viděli, jak se náš agent učil. K tomu budeme potřebovat pomocnou funkci ```show_animation```, která umí zobrazit chování agenta v daném prostředí. Jako parametry má vizualizovaného agenta s implementouvanou metodou ```act```, open AI přístředí, které se má použít, počet simulovaných kroků v epizodě a počet epizod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_animation(agent, env, steps=200, episodes=1):\n",
    "    for i in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        time = 0\n",
    "        reward = 0\n",
    "        while not (done or terminated) and time < steps:\n",
    "            env.render()\n",
    "            action = agent.act(observation, reward, done)\n",
    "            observation, reward, done, terminated, _  = env.step(action)\n",
    "            total_reward += reward\n",
    "            time += 1\n",
    "        agent.reset()\n",
    "\n",
    "def moving_average(x, n):\n",
    "    weights = np.ones(n)/n\n",
    "    return np.convolve(np.asarray(x), weights, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z výsledků vidíme, že se agent moc neučil, což dává smysl, protože nemá implementované žádné rozumné učící se tělo (akce se vybírají náhodně)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mMountainCar-v0\u001b[39m\u001b[39m'\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m show_animation(agent, env, steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, episodes\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(moving_average(total_rewards, \u001b[39m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mshow() \n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mshow_animation\u001b[0;34m(agent, env, steps, episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m     11\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(observation, reward, done)\n\u001b[0;32m---> 12\u001b[0m observation, reward, done, terminated, _  \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     13\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     14\u001b[0m time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/pialg/venv/lib/python3.9/site-packages/gymnasium/wrappers/time_limit.py:51\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/pialg/venv/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:38\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/pialg/venv/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/pialg/venv/lib/python3.9/site-packages/gymnasium/envs/classic_control/mountain_car.py:148\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m (position, velocity)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Documents/pialg/venv/lib/python3.9/site-packages/gymnasium/envs/classic_control/mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    267\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    269\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "show_animation(agent, env, steps=1000, episodes=5)\n",
    "plt.plot(moving_average(total_rewards, 10))\n",
    "plt.show() \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Úkol na cvičení\n",
    "\n",
    "Zkuste si místo náhodného agenta naprogramovat třídu agenta pomocí Q-učení, který se učí chovat v prostředí MountainCar. Dejte pozor na to, že prostředí vrací jako stav spojité hodnoty (poloha i rychlost jsou obě spojité), takže je třeba si z nich nějak udělat prostředí diskrétní (tedy s konečným množstvím stavů). Čím menší budou diskretizované intervaly, tím bude učení přesnější, ale tím déle bude trvat, takže je potřeba najít nějakou rozumnou hranici (ideálně vyzkoušením více hodnot). Dále můžete také experimentovat s dalšími parametry, například měnit maximální počty kroků, případně hodnotu odměny a pozorovat, jak se bude učení měnit.\n",
    "\n",
    "Při implementaci můžete vycházet z následujícího interface (ale nemusíte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateDiscretizer:\n",
    "  # predani rozmeru prostredi a spojitych stavu a jejich rozdeleni na diskretni intervaly\n",
    "    def __init__(self, ranges, states):\n",
    "        pass\n",
    "    \n",
    "    # prirazeni stavu do spravneho intervalu\n",
    "    def transform(self, obs):\n",
    "        pass\n",
    "        \n",
    "class QLearningAgent:\n",
    "    # nastaveni moznych akci - L, N, R   \n",
    "    # diskretizace stavu prostredi\n",
    "    # definice matice uzitku Q[stavy, akce]\n",
    "    # promenna na zapamatovani si minuleho stavu a minule akce\n",
    "    # donastaveni dalsich parametru trenovani\n",
    "    def __init__(self, actions, state_transformer, train=True):\n",
    "        pass\n",
    "    \n",
    "    # na zaklade stavu a akce se vybira nova akce\n",
    "    # 1. najde se nejlepsi akce pro dany stav\n",
    "    # 2. s malou pravd. vezme nahodnou\n",
    "    # 3. updatuje se Q matice\n",
    "    def act(self, observe, reward, done):\n",
    "        pass\n",
    "\n",
    "    # reset minuleho stavu a akce na konci epizody\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "07917789c92289ed904abbd165f8794c0fb764676ab9bc1103c33aff54f03e14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
